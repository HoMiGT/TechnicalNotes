# 卷积
## 恒等卷积: 图像处理后不变

$$\begin{bmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{bmatrix}$$

## 边缘检测: 突出图片矩阵中变化剧烈的位置

$$\begin{bmatrix}
1 & 0 & -1 \\
0 & 0 & 0 \\
-1 & 0 & 1 
\end{bmatrix}$$

对角方向的边缘更容易识别出来

$$\begin{bmatrix}
0 & 1 & 0 \\
1 & -4 & 1 \\
0 & 1 & 0
\end{bmatrix}$$

水平和垂直方向的边缘更容易识别出来

$$\begin{bmatrix}
-1 & -1 & -1 \\
-1 & 8 & -1 \\
-1 & -1 & -1 
\end{bmatrix}$$

水平和倾斜的边缘都容易识别出来

## 锐化卷积

$$\begin{bmatrix}
0 & -1 & 0 \\
-1 & 5 & -1 \\
0 & -1 & 0
\end{bmatrix}$$

当卷积核的权重和大于1时，会整体使图片变亮，小于1会变暗，等于1会保持原始亮度。

## 盒模糊

$$\frac{1}{9}\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 
\end{bmatrix}$$

使图片均匀模糊

## 高斯模糊

$$\frac{1}{16}\begin{bmatrix}
1 & 2 & 1 \\
2 & 4 & 2 \\ 
1 & 2 & 1
\end{bmatrix}$$

3*3的高斯模糊卷积，最后除以权重和16

$$\frac{1}{256}\begin{bmatrix}
1 & 4 & 6 & 4 & 1 \\
4 & 16 & 24 & 16 & 4 \\
6 & 24 & 36 & 24 & 6 \\
4 & 16 & 24 & 16 & 4 \\
1 & 4 & 6 & 4 & 1
\end{bmatrix}$$

5*5的高斯模糊卷积，最后除以权重256

卷积核的值围绕着中心点分布，离中心点越近，贡献越大，权重值越高
高斯模糊的效果比盒模糊更清晰一些

## 反锐化掩膜

$$-\frac{1}{256}\begin{bmatrix}
1 & 4 & 6 & 4 & 1 \\
4 & 16 & 24 & 16 & 4 \\
6 & 24 & -476 & 24 & 6 \\
4 & 16 & 24 & 16 & 4 \\
1 & 4 & 6 & 4 & 1
\end{bmatrix}$$

反锐化掩膜与5阶高斯模糊的卷积核，所有位置都取相反数，中心位置取值476(-36+256+256)，还是达到一种锐化效果


[深度学习参考文章](https://paddlepedia.readthedocs.io/en/latest/index.html)

# 核心概念
## 学习率(LearningRate)
> * 定义：学习率是**控制每次参数更新步长的超参数**，在梯度下降优化过程中决定权重调整的速度。
> * 类比：想象你在夜晚开车去一个陌生的地方：
>   * **学习率太大**: 相当于你**开车速度过快**，可能会直接冲出道路，错过正确的方向，导致训练不稳定甚至发散(Loss上下震荡)
>   * **学习率太小**: 相当于你**开车速度太慢**，虽然方向对了，但需要很久才能到达目的地(收敛速度慢)
>   * **合适的学习率**： 即不会太快导致震荡，也不会太慢影响效率，能平稳收敛到最优点。
> * 如何选择：
>   * 使用学习率衰减：开始时大一些，后面逐步减小
>   * 使用优化器(Adam,RMSprop): 这些优化器可以动态调整学习率
> * 实践经验
>   * 一般初始值： 0.001-0.01，但需要实验调整
>   * SGD通常需要手动调整学习率，而Adam这种优化器可以自适应学习率
## 激活函数(ActivationFunction)
> * 定义：激活函数是**引入非线性能力的函数**，使神经网络能够学习复杂的模式
> * 为什么需要？
>   * 如果没有激活函数(即**仅有线性层**)，即使有多层，最终仍然只是**线性变换**，无法学习复杂的特征。
>   * **非线性激活函数**可以让模型学会**复杂的决策边界**
> ```
> 决策边界(Decision Boundary)：是分类模型用来区分不同类别的边界。它定义了模型在特征空间中每一部分属于哪个类别的规则。
> 具体来说，决策边界将输入数据的特征空间分成不同的区域，每个区域对应一个类别
>
> 简单的例子：
> 假设我们有一个二维的特征空间，数据点由x1和x2俩个特征组成。每个数据点都属于类别A和类别B。决策边界就是一个线或曲线，它将特征空间划分为俩个部分
> * 其中一部分所有数据点被分类为类别A
> * 另一部分所有数据点被分类为类别B
>
> 如何理解决策边界？
> * 在线性分类器(如逻辑回归或线性支持向量机)中，决策边界通常是一个直线或平面。
> * 在非线性分类器(如神经网络，决策树或支持向量机的RBF核)中，决策边界可以是曲线，因为模型可以通过更复杂的方式来划分特征空间
>
> 直观的理解：
> 想象你有一个房间，在这个房间里有俩种颜色的球，红色和蓝色。决策边界就像是地板上画的一条线，它决定了房间的哪一部分属于红球，哪一部分属于篮球。
> 如果你将球放在决策边界的一侧，它就属于红球，放另一边，就属于篮球。
> ```
> * 常见的激活函数
>
> |名称|公式|作用|
> |:--|:--|:--|
> |ReLU(Rectified Linear Unit)| $$f(x) = max(0,x)$$ |解决梯度消失问题，计算简单，广泛用于CNN、MLP|
> |Sigmoid|$$f(x)=\frac{1}{1+e^{-x}}$$|历史上常用，但容易梯度消失|
> |Tanh|$$f(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$|在[-1,1]之间，常用于RNN|
> |Leaky ReLU|$$f(x)=max(0.001x,x)$$|解决ReLU的“死亡神经元”问题|
> |Softmax|$$f(x)_i=\frac{e^{x_i}}{\sum{e^{x_j}}}$$|用于分类任务，输出多个类别的概率|
>
> * 实践经验
>   * 大多数情况：用ReLU(或Leaky ReLU)
>   * 分类任务的最后一层：用Softmax(二分类用Sigmoid)
## 正则化(Regularization)
> * 定义： 正则化是**防止模型过拟合**的一种技术。深度学习模型参数很多，容易记住训练数据(过拟合)，所以需要一些方法增加泛化能力。
> * 常见的正则化方法：
>   * L1正则化(Lasso,L1 penalty)
>     $$L1=\lambda\sum{|w|}$$
>     * 作用： 使某些权重变为0，实现特征选择。
>     * 适用于：稀疏模型(如Lasso回归)。
>   * L2正则化(Ridge,L2 penalty)
>     $$L2=\lambda\sum{w^2}$$
>     * 作用：让权重变得更小但不会变为0，减少过拟合。
>     * 适合于：一般的神经网络模型，默认选L2(如权重衰减weight decay)
>   * Dropout
>     * 训练时**随机丢弃一部分神经元**，防止模型过拟合。
>     * 例如Dropout(0.5),表示**每次训练随机丢弃50%的神经元**。
>   * 数据增强(Data Augmentation)
>     * 在图像任务中**随机翻转**、**裁剪**、**旋转**，让模型学会更多变化，提高泛化能力。
> * 实践经验:   
>   * 绝大多数深度学习任务都使用**L2正则化**(即权重衰减)。
>   * CNN任务通常使用**数据增强**+**Dropout**来防止过拟合。
## 正则化强度(Regularization Strength, $$\lambda$$ )
> * 定义：正则化的强度 $$\lambda$$ 决定了模型对正则项的权重。
> * 类比：
>   * **$$\lambda$$ 过大**：相当于给神经网络“绑手绑脚”，不让它自由调整参数，可能导致欠拟合。
>   * **$$\lambda$$ 过小**：神经网络太自由，容易过拟合。
> * 实践经验:
>   * 一般设为0.0001-0.01,需要调参寻找最佳值。
## 问题类型
> * 问题类型
>   1. 回归问题(Regression)
>     * 任务：预测连续数值(如房价、气温)。
>     * 激活函数：输出层用线性激活
>     * 损失函数: MSE(均方误差)  $$L=\frac{1}{N}\sum{(y_{true}-y_{pred})^2}$$
>   2. 二分类问题(Binary Classification)
>     * 任务: 是/否、真/假、正负分类问题。
>     * 激活函数: Sigmoid
>     * 损失函数: 交叉熵损失(BinaryCrossEntropy)  $$L=-\frac{1}{N}\sum{y_{true}log(y_{pred})+(1-y_{true})log(1-y_{pred})}$$
>   3. 多分类问题(Multi-Class Classification)
>     * 任务: 手写数字识别(0-9)、图片分类(猫、狗、鸟)。
>     * 激活函数：Softmax
>     * 损失函数: 交叉熵损失(Categorical Cross Entropy)  $$L=-\sum{y_ilog(\hat{y}_i)}$$
> * 实践经验
>   * 回归问题 --> MSE
>   * 二分类 --> Sigmoid + 二分类交叉熵
>   * 多分类 --> Softmax + 交叉熵
## 总结
>
> |概念|作用|影响|
> |:--|:--|:--|
> |**学习率( $$\eta$$ )**|控制训练速度|过大会震荡，过小训练慢|
> |**激活函数**|提供非线性能力|ReLU最常用，分类用Softmax|
> |**正则化**|防止过拟合|L2是默认选择|
> |**正则化强度( $$\lambda$$ )**|控制正则化程度|过大欠拟合，过小过拟合|
> |**问题类型**|选择正确的损失函数|分类用交叉熵，回归用MSE|
